---
layout: page
title: Situated interactive agents
short_title: Agents
permalink: /masters-projects/situated-agents/
---

## Goal

Explore how a situated agent can learn language through its interactions with other humans and with its environment.

## Background

Situated agents or robots must be able to interact with the physical environment that are located in with their conversational partner. Such an agent receives information both from its conversational partner and its physical environment which it must integrate in order to learn. Furthermore, since both the world and the language are changeable from one context to another it must be able to adapt to such changes and learn incrementally. Finally, since we have a limited time to train these agents, can information from other domains (for example, models trained on large corpora) be used as a background knowledge?

## Problem description

  - T1. Sensory observations of a robot may be incomplete due to errors that robot's sensors or actuators introduce or simply because the robot has not explored and mapped the entire world yet. Can a robot query a human about the missing knowledge linguistically? How can this information be used with the classification models of its perception and action?
  - T2. What are the interaction strategies in the interactive tutor-robot scenario, how they can be implemented in the ML model and how effective they are?
  - T2. How can a pre-trained knowledge (for example, features trained with deep learning) from a large collection of image/RGB-D data) help the robot learn objects presented to them, and more successfully? What models of machine learning should we use?

## Recommended knowledge and skills

For students in Masters in Language Technology (MLT), the master thesis would build on courses such as (i) Computational Semantics, (ii) Dialogue systems, (iii) Machine Learning, (iv) Embodied and situated language processing, (v) AI: Cognitive Systems or equivalent. For other students, experience with (i) Language Technology, (ii) Machine Learning, (iii) Computer Vision, (vi) Robotics, and (vii) Cognitive Science or equivalent is useful. A potential candidate should be comfortable in programming with Python, have a knowledge of Ubuntu/Linux command shell environment and occasionally be able to solve hardware problems with Kinect and possibly networking. However, help will be provided.

## Supervisors

Simon Dobnik And Nikolai Ilinykh


## Literature / previous work

  1. E. de Graaf. Learning objects and spatial relations with Kinect. Master’s thesis, Department of Philosophy, Linguistics and Theory of Science. University of Gothenburg, Gothenburg, Sweden, June, 8th 2016. Supervisor: Simon Dobnik, examiner: Richard Johansson, opponent: Lorena Llozhi. video
  2. S. Dobnik and E. de Graaf. KILLE: a framework for situated agents for learning language through inter- action. In J. Tiedemann, editor, Proceedings of the 21st Nordic Conference on Computational Linguistics (NoDaLiDa), volume 131 of Linköping Electronic Conference Proceedings and NEALT Proceedings Series Vol. 29, pages 1–10, Gothenburg, Sweden, 22–24 May 2017. Northern European Association for Language Technology (NEALT), Linköping University Electronic Press.
  3. Y. A. Mohammed. Guesswhat?! from what we answered before: Improving the vqa task in goal- oriented games using the previous context of dialogue improving vqa task in goal-oriented games using the previous context of the dialogue. Masters in language technology (mlt), 30 hec, Department of Philosophy, Linguistics and Theory of Science (FLOV), University of Gothenburg, Gothenburg, Sweden, February 3 2020. Supervisor: Simon Dobnik and Mehdi Ghanimifard, examiner: Staffan Larsson.
  4. J. M. Cano Santín. Fast visual grounding in interaction: bringing few-shot learning with neural networks to an interactive robot. Masters in language technology (MLT), 30 hec, Department of Philosophy, Lin- guistics and Theory of Science (FLOV), University of Gothenburg, Gothenburg, Sweden, September 18 2019. Supervisor: Simon Dobnik and Mehdi Ghanimifard, examiner: Aarne Ranta.
  5. J. M. Cano Santín, S. Dobnik, and M. Ghanimifard. Interactive visual grounding with neural networks. In J. Hough, C. Howes, and C. Kennington, editors, Proceedings of LondonLogue – Semdial 2019: The 23rd Workshop on the Semantics and Pragmatics of Dialogue, pages 1–3, London, UK, 4–6 September 2019. Queen Mary University of London.
  6. A. Aruqi. Embodied question answering in robotic environment: Automatic generation of a synthetic question-answer data-set. Masters in language technology (mlt), 30 hec, Department of Philosophy, Linguistics and Theory of Science (FLOV), University of Gothenburg, Gothenburg, Sweden, October 28 2021. Supervisor: Simon Dobnik and Nikolai Ilinykh, examiner: Staffan Larsson, opponent: Catherine Viloria.
