@article{Ahn2022c,
	title = {Do {{As I Can}}, {{Not As I Say}}: {{Grounding Language}} in {{Robotic Affordances}}},
	shorttitle = {Do {{As I Can}}, {{Not As I Say}}},
	author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan},
	year = {2022},
	month = apr,
	journal = {arXiv:2204.01691 [cs]},
	eprint = {2204.01691},
	eprinttype = {arxiv},
	primaryclass = {cs},
	abstract = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's ``hands and eyes,'' while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at say-can.github.io.},
	archiveprefix = {arXiv},
	langid = {english},
	keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics}
}

@article{Lee:2022tm,
	author = {Lee, Jae Hee and Kerzel, Matthias and Ahrens, Kyra and Weber, Cornelius and Wermter, Stefan},
	journal = {arXiv},
	title = {What is Right for Me is Not Yet Right for You: A Dataset for Grounding Relative Directions via Multi-Task Learning},
	volume = {arXiv:2205.02671 [cs.CV]},
	year = {2022},
	url = {https://arxiv.org/abs/2205.02671}}
	
@article{Schlangen:2022ux,
	author = {Schlangen, David},
	doi = {10.48550/ARXIV.2206.02885},
	journal = {arXiv},
	title = {Norm Participation Grounds Language},
	url = {https://arxiv.org/abs/2206.02885},
	volume = {arXiv:2206.02885 [cs.CL]},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2206.02885},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2206.02885},
	bdsk-url-3 = {https://arxiv.org/pdf/2206.02885.pdf}}
	
@misc{https://doi.org/10.48550/arxiv.2207.14000,
  doi = {10.48550/ARXIV.2207.14000},
  
  url = {https://arxiv.org/abs/2207.14000},
  
  author = {Bao, Qiming and Peng, Alex Yuxuan and Hartill, Tim and Tan, Neset and Deng, Zhenyun and Witbrock, Michael and Liu, Jiamou},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Logic in Computer Science (cs.LO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
