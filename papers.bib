

@misc{clark2021old,
      title={Something Old, Something New: Grammar-based CCG Parsing with Transformer Models}, 
      author={Stephen Clark},
      year={2021},
      eprint={2109.10044},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{deyo2023logical,
      title={A logical word embedding for learning grammar}, 
      author={Sean Deyo and Veit Elser},
      year={2023},
      eprint={2304.14590},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Ahn2022c,
	title = {Do {{As I Can}}, {{Not As I Say}}: {{Grounding Language}} in {{Robotic Affordances}}},
	shorttitle = {Do {{As I Can}}, {{Not As I Say}}},
	author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan},
	year = {2022},
	month = apr,
	journal = {arXiv:2204.01691 [cs]},
	eprint = {2204.01691},
	eprinttype = {arxiv},
	primaryclass = {cs},
	abstract = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's ``hands and eyes,'' while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at say-can.github.io.},
	archiveprefix = {arXiv},
	langid = {english},
	keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics}
}

@article{Lee:2022tm,
	author = {Lee, Jae Hee and Kerzel, Matthias and Ahrens, Kyra and Weber, Cornelius and Wermter, Stefan},
	journal = {arXiv},
	title = {What is Right for Me is Not Yet Right for You: A Dataset for Grounding Relative Directions via Multi-Task Learning},
	volume = {arXiv:2205.02671 [cs.CV]},
	year = {2022},
	url = {https://arxiv.org/abs/2205.02671}}
	
@article{Schlangen:2022ux,
	author = {Schlangen, David},
	doi = {10.48550/ARXIV.2206.02885},
	journal = {arXiv},
	title = {Norm Participation Grounds Language},
	url = {https://arxiv.org/abs/2206.02885},
	volume = {arXiv:2206.02885 [cs.CL]},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2206.02885},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2206.02885},
	bdsk-url-3 = {https://arxiv.org/pdf/2206.02885.pdf}}

	
@misc{https://doi.org/10.48550/arxiv.2207.14000,
  doi = {10.48550/ARXIV.2207.14000},
  
  url = {https://arxiv.org/abs/2207.14000},
  
  author = {Bao, Qiming and Peng, Alex Yuxuan and Hartill, Tim and Tan, Neset and Deng, Zhenyun and Witbrock, Michael and Liu, Jiamou},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Logic in Computer Science (cs.LO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{brunila-laviolette-2022-company,
    title = "What company do words keep? Revisiting the distributional semantics of {J}.{R}. Firth {\&} Zellig {H}arris",
    author = "Brunila, Mikael  and
      LaViolette, Jack",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.327",
    doi = "10.18653/v1/2022.naacl-main.327",
    pages = "4403--4417",
    abstract = "The power of word embeddings is attributed to the linguistic theory that similar words will appear in similar contexts. This idea is specifically invoked by noting that {``}you shall know a word by the company it keeps,{''} a quote from British linguist J.R. Firth who, along with his American colleague Zellig Harris, is often credited with the invention of {``}distributional semantics.{''} While both Firth and Harris are cited in all major NLP textbooks and many foundational papers, the content and differences between their theories is seldom discussed. Engaging in a close reading of their work, we discover two distinct and in many ways divergent theories of meaning. One focuses exclusively on the internal workings of linguistic forms, while the other invites us to consider words in new company{---}not just with other linguistic elements, but also in a broader cultural and situational context. Contrasting these theories from the perspective of current debates in NLP, we discover in Firth a figure who could guide the field towards a more culturally grounded notion of semantics. We consider how an expanded notion of {``}context{''} might be modeled in practice through two different strategies: comparative stratification and syntagmatic extension.",
}

@misc{https://doi.org/10.48550/arxiv.2212.01681,
  doi = {10.48550/ARXIV.2212.01681},
  url = {https://arxiv.org/abs/2212.01681},
  author = {Andreas, Jacob},
  keywords = {Computation and Language (cs.CL), Multiagent Systems (cs.MA), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Language Models as Agent Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@misc{https://doi.org/10.48550/arxiv.2212.09736,
  doi = {10.48550/ARXIV.2212.09736},
  
  url = {https://arxiv.org/abs/2212.09736},
  
  author = {Gu, Yu and Deng, Xiang and Su, Yu},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7},
  
  title = {Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{10.1371/journal.pone.0281372,
    doi = {10.1371/journal.pone.0281372},
    author = {Betz, Gregor AND Richardson, Kyle},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Probabilistic coherence, logical consistency, and Bayesian learning: Neural language models as epistemic agents},
    year = {2023},
    month = {02},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pone.0281372},
    pages = {1-29},
    abstract = {It is argued that suitably trained neural language models exhibit key properties of epistemic agency: they hold probabilistically coherent and logically consistent degrees of belief, which they can rationally revise in the face of novel evidence. To this purpose, we conduct computational experiments with rankers: T5 models [Raffel et al. 2020] that are pretrained on carefully designed synthetic corpora. Moreover, we introduce a procedure for eliciting a modelâ€™s degrees of belief, and define numerical metrics that measure the extent to which given degrees of belief violate (probabilistic, logical, and Bayesian) rationality constraints. While pretrained rankers are found to suffer from global inconsistency (in agreement with, e.g., [Jang et al. 2021]), we observe that subsequent self-training on auto-generated texts allows rankers to gradually obtain a probabilistically coherent belief system that is aligned with logical constraints. In addition, such self-training is found to have a pivotal role in rational evidential learning, too, for it seems to enable rankers to propagate a novel evidence item through their belief systems, successively re-adjusting individual degrees of belief. All this, we conclude, confirms the Rationality Hypothesis, i.e., the claim that suitable trained NLMs may exhibit advanced rational skills. We suggest that this hypothesis has empirical, yet also normative and conceptual ramifications far beyond the practical linguistic problems NLMs have originally been designed to solve.},
    number = {2},

}

@misc{https://doi.org/10.48550/arxiv.2212.03551,
     doi = {10.48550/ARXIV.2212.03551},
     url = {https://arxiv.org/abs/2212.03551},
     author = {Shanahan, Murray},
     keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
     title = {Talking About Large Language Models},
     publisher = {arXiv},
     year = {2022},
     copyright = {Creative Commons Attribution 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2301.12507,
  doi = {10.48550/ARXIV.2301.12507},
  
  url = {https://arxiv.org/abs/2301.12507},
  
  author = {Sumers, Theodore and Marino, Kenneth and Ahuja, Arun and Fergus, Rob and Dasgupta, Ishita},
  
  keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Distilling Internet-Scale Vision-Language Models into Embodied Agents},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{ebert-etal-2022-trajectories,
    title = "Do Trajectories Encode Verb Meaning?",
    author = "Ebert, Dylan  and
      Sun, Chen  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.206",
    doi = "10.18653/v1/2022.naacl-main.206",
    pages = "2860--2871",
    abstract = "Distributional models learn representations of words from text, but are criticized for their lack of grounding, or the linking of text to the non-linguistic world. Grounded language models have had success in learning to connect concrete categories like nouns and adjectives to the world via images and videos, but can struggle to isolate the meaning of the verbs themselves from the context in which they typically occur. In this paper, we investigate the extent to which trajectories (i.e. the position and rotation of objects over time) naturally encode verb semantics. We build a procedurally generated agent-object-interaction dataset, obtain human annotations for the verbs that occur in this data, and compare several methods for representation learning given the trajectories. We find that trajectories correlate as-is with some verbs (e.g., fall), and that additional abstraction via self-supervised pretraining can further capture nuanced differences in verb meaning (e.g., roll and slide).",
}

