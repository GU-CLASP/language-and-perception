---
layout: page
title: Papers in the attic
---

Here is a list of papers that were on our list to read but they did not make it in the schedule. Please feel free to move back any of them to the list of suggestions if they are (or become) of interest.

  * Author. Year. Paper.
  
  *  J. H. Lee, M. Kerzel, K. Ahrens, C. Weber, and S. Wermter. [What is right for me is not yet right for you: A dataset for grounding relative directions via multi-task learning.](https://arxiv.org/abs/2205.02671) arXiv, arXiv:2205.02671 [cs.CV], 2022. [Dataset](https://github.com/knowledgetechnologyuhh/grid-3d)
  
  *  Y. Liu and G. Emerson. [Learning functional distributional semantics with visual data.](https://aclanthology.org/2022.acl-long.275) In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3976–3988, Dublin, Ireland, May 2022. Association for Computational Linguistics.
  
  *  F. Liu, G. Emerson, and N. Collier. [Visual spatial reasoning.](https://arxiv.org/abs/2205.00363) arXiv, arXiv:2205.00363 [cs.CL], 2022.
  
  *  E. Bugliarello, R. Cotterell, N. Okazaki, and D. Elliott. [Multimodal pretraining unmasked: A meta- analysis and a unified framework of vision-and-language BERTs.](https://aclanthology.org/2021.tacl-1.58) Transactions of the Association for Computational Linguistics, 9:978–994, 2021.
  
  *  L. W. Barsalou. [Grounded cognition.](https://doi.org/10.1146/annurev.psych.59.103006.093639) Annual Review of Psychology, 59:617–645, 2008.
  
  *  R. Bernardi, R. Cakici, D. Elliott, A. Erdem, E. Erdem, N. Ikizler-Cinbis, F. Keller, A. Muscat, and B. Plank. [Automatic description generation from images: A survey of models, datasets, and evaluation measures.](https://doi.org/10.1613/jair.4900) Journal of Artificial Intelligence Research, 55:409–442, 2016.
  
  *  R. Bernardi and S. Pezzelle. [Linguistic issues behind visual question answering.](https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/lnc3.12417) Language and Linguistics Compass, 15(6):elnc3.12417, 2021.
  
  *  S. Buch, L. Fei-Fei, and N. D. Goodman. [Neural Event Semantics for Grounded Language Understanding.](https://doi.org/10.1162/tacl\_a\_00402) Transactions of the Association for Computational Linguistics, 9:875–890, 08 2021.
  
  *  J. Cho, J. Lei, H. Tan, and M. Bansal. [Unifying vision-and-language tasks via text generation.](https://arxiv.org/abs/2102.02779) arXiv, arXiv:2102.02779 [cs.CL], 2021.
  
  *  G. Collell and M.-F. Moens. [Learning representations specialized in spatial knowledge: Leveraging language and vision.](https://www.transacl.org/ojs/index.php/tacl/article/view/1214) Transactions of the Association for Computational Linguistics, 6:133–144, 2018.
  
  *  I. Dasgupta, C. Kaeser-Chen, K. Marino, A. Ahuja, S. Babayan, F. Hill, and R. Fergus. [Collaborating with language models for embodied reasoning.](https://arxiv.org/abs/2302.00763) arXiv, arXiv:2302.00763 [cs.LG], 2023.
  
  *  R. Dess`ı, E. Gualdoni, F. Franzon, G. Boleda, and M. Baroni. [Communication breakdown: On the low mutual intelligibility between human and neural captioning.](https://arxiv.org/abs/2210.11512) arXiv, arXiv:2210.11512 [cs.CL], 2022.
  
  *  T. Dong, A. Testoni, L. Benotti, and R. Bernardi. [Visually grounded follow-up questions: a dataset of spatial questions which require dialogue history.](https://aclanthology.org/2021.splurobonlp-1.3) In Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics, pages 22–31, Online, Aug. 2021. Association for Computational Linguistics.
  
  *  C. Greco, B. Plank, R. Fern ́andez, and R. Bernardi. [Psycholinguistics meets continual learning: Measuring catastrophic forgetting in visual question answering.](https://www.aclweb.org/anthology/P19-1350) In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3601–3605, Florence, Italy, July 2019. Association for Computational Linguistics.
  
  *  E. Gualdoni, T. Brochhagen, A. Mädebach, and G. Boleda. [What’s in a name? a large-scale computational study on how competition between names affects naming variation.](https://psyarxiv.com/t84eu/download) Submitted, 2023.
  
  *  J. Lu, D. Batra, D. Parikh, and S. Lee. [Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.](http://arxiv.org/abs/1908.02265) arXiv, arXiv:1908.02265 [cs.CV], 2019.
  
  *  J. Lu, V. Goswami, M. Rohrbach, D. Parikh, and S. Lee. [12-in-1: Multi-task vision and language representation learning.](https://openaccess.thecvf.com/content_CVPR_2020/html/Lu_12-in-1_Multi-Task_Vision_and_Language_Representation_Learning_CVPR_2020_paper.html) In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–10, June 2020.
  
  *  I. Parfenova, D. Elliott, R. Fern ́andez, and S. Pezzelle. [Probing cross-modal representations in multi- step relational reasoning.](https://aclanthology.org/2021.repl4nlp-1.16) In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 152–162, Online, Aug. 2021. Association for Computational Linguistics.
  
  *  A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. [Learning transferable visual models from natural language supervision.](https://proceedings.mlr.press/v139/radford21a.html) In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748–8763. PMLR, 18–24 Jul 2021.
  
  *  T. Ramalho, T. Kocisky ́, F. Besse, S. M. A. Eslami, G. Melis, F. Viola, P. Blunsom, and K. M. Hermann. [Encoding spatial relations from natural language.](http://arxiv.org/abs/1807.01670) arXiv, arXiv:1807.01670 [cs.CL]:16, July 5 2018.
  
  *  F. Sadeghi, S. K. Kumar Divvala, and A. Farhadi. [Viske: Visual knowledge extraction and question answering by visual verification of relation phrases.](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Sadeghi_VisKE_Visual_Knowledge_2015_CVPR_paper.html) In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1456–1464, 2015.
  
  *  S. Shen, L. H. Li, H. Tan, M. Bansal, A. Rohrbach, K. Chang, Z. Yao, and K. Keutzer. [How much can CLIP benefit vision-and-language tasks?](https://arxiv.org/abs/2107.06383) arXiv, arXiv:2107.06383 [cs.CV], 2021.
  
  *  C. Silberer, S. Zarrieß, M. Westera, and G. Boleda. [Humans meet models on object naming: A new dataset and analysis.](https://aclanthology.org/2020.coling-main.172) In Proceedings of the 28th International Conference on Computational Linguistics, pages 1893–1905, Barcelona, Spain (Online), Dec. 2020. International Committee on Computational Linguistics.
  
  *  A. Singh, R. Hu, V. Goswami, G. Couairon, W. Galuba, M. Rohrbach, and D. Kiela. [Flava: A founda- tional language and vision alignment model.](https://openaccess.thecvf.com/content/CVPR2022/html/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.html) In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15638–15650, June 2022.
  
  *  G. Skantze and B. Willemsen. [CoLLIE: Continual learning of language grounding from language-image embeddings.](https://doi.org/10.1613%2Fjair.1.13689) Journal of Artificial Intelligence Research, 74:1201–1223, jul 2022.
  
  *  E. Sood, F. K ̈ogel, F. Strohm, P. Dhar, and A. Bulling. [VQA-MHUG: A gaze dataset to study multimodal neural attention in visual question answering.](https://aclanthology.org/2021.conll-1.3) In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 27–43, Online, Nov. 2021. Association for Computational Linguistics.
  
  *  W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai. [VL-BERT: Pre-training of generic visual- linguistic representations.](https://arxiv.org/abs/1908.08530) arXiv, arXiv:1908.08530 [cs.CV], 2019.
  
  *  H. Tan and M. Bansal. [LXMERT: Learning cross-modality encoder representations from transformers.](https://www.aclweb.org/anthology/D19-1514) In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5100–5111, Hong Kong, China, Nov. 2019. Association for Computational Linguistics.
  
  *  M. Tsimpoukelli, J. L. Menick, S. Cabi, S. M. A. Eslami, O. Vinyals, and F. Hill. [Multimodal few- shot learning with frozen language models.](https://proceedings.neurips.cc/paper/2021/file/01b7575c38dac42f3cfb7d500438b875-Paper.pdf) In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 200–212. Curran Associates, Inc., 2021.
  
  *  P. Villalobos, J. Sevilla, L. Heim, T. Besiroglu, M. Hobbhahn, and A. Ho. [Will we run out of data? an analysis of the limits of scaling datasets in machine learning.](https://doi.org/10.48550/arXiv.2211.04325) arXiv, arXiv:2211.04325 [cs.LG], 2022.
  
  *  V. Wang-Mascianica and B. Coecke. [Talking space: inference from spatial linguistic meanings.](https://doi.org/10.48550/arXiv.2109.06554) arXiv, arXiv:2109.06554 [cs.CL]:1–33, September 16 2021.
  
  *  M. Zare, A. Ayub, A. Liu, S. Sudhakara, A. Wagner, and R. Passonneau. [Dialogue policies for learning board games through multimodal communication.](https://aclanthology.org/2020.sigdial-1.41) In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 339–351, 1st virtual meeting, July 2020. Association for Computational Linguistics.
  
  *  Z. Zhang, Y. Wang, Q. Wu, and F. Chen. [Visual relationship attention for image captioning.](https://ieeexplore.ieee.org/abstract/document/8851832) In 2019 International Joint Conference on Neural Networks (IJCNN), pages 1–8, July 2019.
  
  *  C. Zheng, Q. Guo, and P. Kordjamshidi.  [Cross-modality relevance for reasoning on language and vision.](https://aclanthology.org/2020.acl-main.683) In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7642– 7651, Online, July 2020. Association for Computational Linguistics.



  * Pay Attention to MLPs. Hanxiao Liu, Zihang Dai, David R. So, Quoc V. Le. (2021). [paper](https://arxiv.org/abs/2105.08050) (from Aram)
    
  * Visually Grounded Follow-up Questions: a Dataset of Spatial Questions Which Require Dialogue History. Dong, T., Testoni, A., Benotti, L., & Bernardi, R. (2021). In Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics (pp. 22–31). Association for Computational Linguistics. [paper](https://aclanthology.org/2021.splurobonlp-1.3.pdf) (from Nikolai)

  * PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World. Zellers, A., Peters, M., Mottaghi, R., Kembhavi, A., Farhadi, A., & Choi, Y. (2021). In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (pp. 2040–2050). Association for Computational Linguistics. [paper](https://aclanthology.org/2021.acl-long.159.pdf) (from Nikolai)

  * A Cognitive Regularizer for Language Modeling. Jason Wei, Clara Meister, Ryan Cottorell. [paper](https://arxiv.org/pdf/2105.07144.pdf)

  * NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning. Zeming Chen, Qiyue Gao, Lawrence S. Moss [paper](https://arxiv.org/abs/2105.14167) (from Adam)

  * Tifrea, A., Bécigneul, G., & Ganea, O.-E. (2018). Poincar\’e GloVe: Hyperbolic Word Embeddings. ArXiv:1810.06546 [Cs]. http://arxiv.org/abs/1810.06546
(from Bill, would like to read: Adam)

  * Mittelman, R., Sun, M., Kuipers, B., & Savarese, S. (2014). A Bayesian generative model for learning semantic hierarchies. Frontiers in Psychology, 5. https://doi.org/10.3389/fpsyg.2014.00417 (from Bill, would like to read: Adam)

  * Emerson, G. (2020). What are the Goals of Distributional Semantics? Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 7436–7453. https://doi.org/10.18653/v1/2020.acl-main.663 (from Bill, would like to read: Adam)

  * Nguyen, D., Rosseel, L., & Grieve, J. (2021). On learning and representing social meaning in NLP: A sociolinguistic perspective. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 603–612. https://www.aclweb.org/anthology/2021.naacl-main.50 (from Bill)

  * Superbizarre Is Not Superb: Derivational Morphology Improves BERT's Interpretation of Complex Words. Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Schütze [paper](https://arxiv.org/abs/2101.00403) (from Adam)

  * A Mutual Information Maximization Perspective of Language Representation Learning. Lingpeng Kong, Cyprien de Masson d'Autume, Wang Ling, Lei Yu, Zihang Dai, Dani Yogatama. [paper](https://arxiv.org/abs/1910.08350) (from Adam, seems reaaaally cool!)

  * Self-Supervised Dialogue Learning. Jiawei Wu, Xin Wang, William Yang Wang. [paper](https://arxiv.org/abs/1907.00448) (from Adam)

  * Norm-Based Curriculum Learning for Neural Machine Translation. Xuebo Liu, Houtim Lai, Derek F. Wong, Lidia S. Chao [paper](https://arxiv.org/abs/2006.02014) (from Adam)

  * Curriculum learning.  Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason. [paper](https://arxiv.org/abs/1704.03003) (from Adam)

  * Automated Curriculum Learning for Neural Networks. Alex Graves, Marc G. Bellemare, Jacob Menick, Remi Munos, Koray Kavukcuoglu [paper](https://arxiv.org/abs/1704.03003) (from Adam)

  * Unsupervised Bilingual Lexicon Induction via Latent Variable Models. Zi-Yi Dou, Zhi-Hao Zhou, Shujian Huang [paper](https://www.aclweb.org/anthology/D18-1062/) (from Adam)

  * Generating Sentences from Disentangled Syntactic and Semantic Spaces. Yu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, Xinyu Dai, Jiajun Chen [paper](https://arxiv.org/pdf/1907.05789.pdf) (from Adam)

  * Residual Energy-Based Models for Text Generation. Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, Marc'Aurelio Ranzato [paper](https://arxiv.org/abs/2004.11714) (from Adam)

  * Generating Sentences from a Continuous Space. Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, Samy Bengio [paper](https://arxiv.org/abs/1511.06349) (from Adam)

  * Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information. Li, J., Tan, H., & Bansal, M. (2021). In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 1041–1050). Association for Computational Linguistics. [paper](https://www.aclweb.org/anthology/2021.naacl-main.82.pdf) (Recommended by Nikolai, would like to read: Nikolai)

  * Measuring Social Biases in Grounded Vision and Language Embeddings. Ross, C., Katz, B., & Barbu, A. (2021). In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 998–1008). Association for Computational Linguistics. [paper](https://www.aclweb.org/anthology/2021.naacl-main.78.pdf) (Recommended by Nikolai, would like to read: Nikolai)

  * KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA. Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, & Marcus Rohrbach. (2020).  CVPR 2021. [paper](https://arxiv.org/pdf/2012.11014.pdf) (Recommended by Nikolai, would like to read: Nikolai)

  * Implicit Representations of Meaning in Neural Language Models. Belinda Z. Li, Maxwell Nye, & Jacob Andreas. (2021). ACL 2021. [paper](https://arxiv.org/pdf/2106.00737.pdf). (Recommended by Nikolai, would like to read: Nikolai)

  * Psycholinguistics Meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering. Greco, C., Plank, B., Fernández, R., & Bernardi, R. (2019). [paper](https://www.aclweb.org/anthology/P19-1350/) In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 3601–3605). Association for Computational Linguistics. (Recommended by Nikolai, would like to read: Nikolai)

  * Incorporating Structural Alignment Biases into an Attentional Neural Translation Model (2021) Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova, Kaisheng Yao, Chris Dyer, Gholamreza Haffari [paper](https://www.aclweb.org/anthology/N16-1102/) (Recommended by Adam, would like to read: Adam)

  * How (Non-)Optimal is the Lexicon? (2021) Tiago Pimentel, Irene Nikkarinen, Kyle Mahowald, Ryan Cotterell, Damián Blasi [paper](https://arxiv.org/abs/2104.14279) (Recommended by Adam, would like to read: Adam)

  * Benotti, L., & Blackburn, P. (2021). Grounding as a Collaborative Process. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume (pp. 515–531). Association for Computational Linguistics. [paper] (https://www.aclweb.org/anthology/2021.eacl-main.41.pdf)  (Recommended by Nikolai, would like to read: Nikolai)

  * Liunian Harold Li, Haoxuan You, Zhecan Wang, Alireza Zareian, Shih-Fu Chang, & Kai-Wei Chang. (2021). Weakly-Supervised VisualBERT: Pre-training Vision-and-Language Representations Without Parallel Images and Captions. [paper](https://arxiv.org/pdf/2010.12831.pdf) NAACL 2021 (Recommended by Nikolai, would like to read: Nikolai)

  * Annette Rios, Chantal Amrhein, Noëmi Aepli, & Rico Sennrich. (2021). On Biasing Transformer Attention Towards Monotonicity. [paper](https://arxiv.org/pdf/2104.03945.pdf) NAACL 2021 (Recommended by Nikolai, would like to read: Nikolai)

  * Spliethöver, M., & Wachsmuth, H. (2020). Argument from Old Man's View: Assessing Social Bias in Argumentation.  (https://www.aclweb.org/anthology/2020.argmining-1.9.pdf) (Recommended by Anna, would like to read: Anna)

  * Kevin Lu, Aditya Grover, Pieter Abbeel, & Igor Mordatch. (2021). Pretrained Transformers as Universal Computation Engines. [paper](https://arxiv.org/pdf/2103.05247.pdf) (Recommended by Nikolai, would like to read: Nikolai)

  * M. Artetxe, G. Labaka, and E. Agirre. (2019). [Bilingual Lexicon Induction through Unsupervised Machine Translation](https://arxiv.org/abs/1907.10761) ACL 2019 (recommended by Adam, would like to read: Simon, Adam, Anna, Nikolai)
  
  * Goodman, N. D., & Stuhlmüller, A. (2013). Knowledge and Implicature: Modeling Language Understanding as Social Cognition. Topics in Cognitive Science. [paper](https://web.stanford.edu/~ngoodman/papers/GS-TopiCS-2013.pdf) (recommended by Bill, would like to read: Robin, Simon, Anna)

  * K. Clark, U. Khandelwal, O. Levy, and C. D. Manning. [What does BERT look at? an analysis of BERT’s attention.](https://www.aclweb.org/anthology/W19-4828) In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276–286, Florence, Italy, Aug. 2019. Association for Computational Linguistics. (recommended by Felix, would like to read: Adam, Anna, Nikolai)

  * J. Bastings and K. Filippova. [The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?](https://www.aclweb.org/anthology/2020.blackboxnlp-1.14) In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 149–155, Online, Nov. 2020. Association for Computational Linguistics. (recommended by Felix, would like to read: Adam, Anna, Nikolai)

  * Licheng Yu, Xinlei Chen, Georgia Gkioxari, Mohit Bansal, Tamara L. Berg, & Dhruv Batra. (2019). Multi-Target Embodied Question Answering (recommended by Simon and Nikolai, would like to read: Simon, Nikolai)

  * Moro, D., Black, S., & Kennington, C. (2019). Composing and Embedding the Words-as-Classifiers Model of Grounded Semantics. arXiv preprint arXiv:1911.03283. (https://arxiv.org/pdf/1911.03283.pdf) (recommended by Staffan, would like to read: Robin, Nikolai)

  * Thomason, J., Padmakumar, A., Sinapov, J., Walker, N., Jiang, Y., Yedidsion, H., ... & Mooney, R. J. (2020). Jointly improving parsing and perception for natural language commands through human-robot dialog. Journal of Artificial Intelligence Research, 67, 1-48. [paper](https://jair.org/index.php/jair/article/view/11485/26562) (recommended by Mehdi, would like to read: Robin, Simon)

  * J. Thomason, M. Murray, M. Cakmak, and L. Zettlemoyer. Vision-and-dialog navigation. In Conference on Robot Learning (CoRL), 2019. [paper](https://arxiv.org/abs/1907.04957) (recommended by Simon, would like to read: Simon, Nikolai)

  * M. Janner, K. Narasimhan, and R. Barzilay. Representation learning for grounded spatial reasoning. Transactions of the Association for Computational Linguistics, 6:49–61, 2018. (recommended by Mehdi, would like to read: Simon, Nikolai) [link](https://www.transacl.org/ojs/index.php/tacl/article/view/1234)

  * Caglayan, O., Ive, J., Haralampieva, V., Madhyastha, P., Barrault, L., & Specia, L. (2020). [Simultaneous Machine Translation with Visual Context.](http://arxiv.org/abs/2009.07310). EMNLP 2020. (recommended by Nikolai, would like to read: Simon)

  * Malt, B. C., Sloman, S. A., Gennari, S., Shi, M., & Wang, Y. (1999). Knowing versus naming: Similarity and the linguistic categorization of artifacts. Journal of Memory and Language, 40(2), 230-262. [paper](https://1b7a2906-a-62cb3a1a-s-sites.googlegroups.com/site/slomanlab/malt_et_al_jml.pdf?attachauth=ANoY7co8b_1l8eshBxSMrD65NRkjgq0cFvJZ-9XwdEVW9r2gOx-a8rOnyZPPV19Ngnx8x4hD2M7huNSJ8NlKz1XV6Zok8U9QutTZgyfoz4AB_k9hVNMjbmk3Skd1vFCOXTiW4f_PuvL3nQxEgVud3jdUqP_9nYWmNuL0aiMYohD9Kq8DiVFd1ywotrzNsTYzM0k5hygIkNyADjIXTGxZnaT80bobVBNp5Q%3D%3D&attredirects=0) (recommended by Staffan, would like to read: Robin)

  * Mollica, F. et al. (2019). Composition is the core driver of the language-selective network [Paper](https://www.biorxiv.org/content/10.1101/436204v2) (recommended by Mehdi, would like to read: Robin)

  * Sellam, T., Das, D., & Parikh, A. P. (2020). BLEURT: Learning Robust Metrics for Text Generation. https://arxiv.org/abs/2004.04696 (recommended by Nikolai, would like to read: Simon)

  * Tan, H., & Bansal, M. (2019). LXMERT: Learning Cross-Modality Encoder Representations from Transformers. https://arxiv.org/abs/1908.07490 (recommended by Simon, would like to read: Simon)

  * Tan, H., Dernoncourt, F., Lin, Z., Bui, T., & Bansal, M. (2019). Expressing Visual Relationships via Language. [paper](https://www.aclweb.org/anthology/P19-1182.pdf) (recommended by Nikolai, would like to read: Simon)

  * J. Zwarts and Y. Winter. Vector space semantics: A model-theoretic analysis of locative prepositions. Journal of Logic, Language and Information, 9:169–211, 2000. (recommended by all, would like to read: Robin)

  * Magnus Sahlgren, Fredrik Carlsson (2020) [The Singleton Fallacy: Why Current Critiques of Language Models Miss the Point](https://arxiv.org/pdf/2102.04310.pdf)

  * Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021). [Learning transferable visual models from natural language supervision.](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf) Image, 2, T2. (recommended by Mehdi)
https://arxiv.org/abs/1609.02116
    
  * Aitor Ormazabal,  Mikel Artetxe,  Aitor Soroa,  Gorka Labaka,  Eneko Agirre [Beyond Offline Mapping: Learning Cross Lingual Word Embeddings through Context Anchoring](https://arxiv.org/pdf/2012.15715.pdf) (Recommended by Adam)
 
  * Ben Bogin, Sanjay Subramanian, Matt Gardner, Jonathan Berant (2020) [Latent Compositional Representations Improve Systematic Generalization in Grounded Question Answering](https://arxiv.org/pdf/2007.00266.pdf) (Recommended by Adam)

  * Paula Czarnowska, Sebastian Ruder, Ryan Cotterell, Ann Copestake (2020) [Morphologically Aware Word-Level Translation](https://arxiv.org/pdf/2011.07593.pdf) (Recommended by Adam)

  * Wu, J., & Mooney, R. J. (2019). Self-Critical Reasoning for Robust Visual Question Answering. http://arxiv.org/abs/1905.09998 (recommended by Simon)

  * Akbik, Alan & Blythe, Duncan & Vollgraf, Roland. (2018). Contextual String Embeddings for Sequence Labeling. 
  [paper](https://www.aclweb.org/anthology/C18-1139/) (recommended by Axel)

  * Wang, Bin & Chen, Fenxiao & Wang, Yuncheng & Kuo, C.. (2020). Efficient Sentence Embedding via Semantic Subspace Analysis. [paper](https://arxiv.org/abs/2002.09620) (recommended by Axel)

  * Marcus, G. (2018). Deep learning: A critical appraisal. [paper](https://arxiv.org/pdf/1801.00631.pdf); [video comments](https://www.youtube.com/watch?v=wh_IZNHH2S0); (recommended by Mehdi)
  
  * J. A. Bateman, M. Pomarlan, and G. Kazhoyan. Embodied contextualization: Towards a multistratal ontological treatment. Applied Ontology, Pre-press:1–35, 2 October 2019. [paper](https://content.iospress.com/articles/applied-ontology/ao190218)

  * [What are the differences between neural networks and the brain?](https://youtu.be/P4wI938mx00) panel discussion from 
Center for Brains, Minds and Machines (CBMM) (recommended by Mehdi)

  * W. N. Havard, J.-P. Chevrot, and L. Besacier. Models of visually grounded speech signal pay attention to nouns: a bilingual experiment on english and japanese. arXic, arXiv:1902.03052 [cs.CL]:1–5, 2019. [paper](https://arxiv.org/abs/1902.03052) (recommended by Sylvie)

  * L. Arras, F. Horn, G. Montavon, K.-R. Müller, and W. Samek. ”What is relevant in a text document?”: An interpretable machine learning approach. PLOS ONE, 12(8):1–23, 08 2017. [paper](https://doi.org/10.1371/journal.pone.0181142) (recommended by Felix)

  * Yatskar, M., Zettlemoyer, L., & Farhadi, A. (2016). Situation recognition: Visual semantic role labeling for image understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 5534-5542). [link](https://homes.cs.washington.edu/~my89/publications/situations.pdf) (recommended by Mehdi)
  
  * Mei, H., Bansal, M., & Walter, M. R. (2016, February). Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences. In AAAI (pp. 2772-2778). [link](https://arxiv.org/abs/1506.04089) (recommended by Mehdi)

  * one of the papers on this page (Oxford robotics & vision group): [link](http://www.robots.ox.ac.uk/~nsid/publications.html) (recommended by Staffan)
  Link broken

  * Ben-Yosef, G., Assif, L., & Ullman, S. (2018). Full interpretation of minimal images. Cognition, 171, 65-84.
[link](https://perso.telecom-paristech.fr/bloch/AIC/articles/BenYosef2017.pdf)
[video](https://www.youtube.com/watch?v=2TWS_d843ys)
(recommended by Mehdi)
